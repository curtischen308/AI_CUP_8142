# run_experiments.py
from __future__ import annotations
import argparse
from pathlib import Path
from dataclasses import dataclass, asdict
from typing import Dict, List, Tuple

import numpy as np
import pandas as pd
import xgboost as xgb
from sklearn.model_selection import train_test_split
from sklearn.metrics import average_precision_score, precision_recall_curve

# ---------------- Utils ----------------
def _encode_object_columns(df: pd.DataFrame) -> pd.DataFrame:
    """æŠŠ object é¡å‹ç”¨ factorize è½‰æˆæ•´æ•¸ï¼Œé¿å… DMatrix dtype å•é¡Œã€‚"""
    df = df.copy()
    for c in df.columns:
        if df[c].dtype == "object":
            df[c] = pd.factorize(df[c])[0]
    return df

def _pick_device_params() -> Dict[str, str]:
    """å„ªå…ˆç”¨ GPUï¼Œç„¡æ³•ä½¿ç”¨å°±é€€å› CPUã€‚"""
    try:
        params = {"objective": "binary:logistic", "tree_method": "hist", "device": "cuda"}
        dm = xgb.DMatrix(np.array([[0.0], [1.0]]), label=np.array([0, 1]))
        xgb.train(params, dm, num_boost_round=1)
        return {"tree_method": "hist", "device": "cuda"}
    except Exception:
        return {"tree_method": "hist", "device": "cpu"}

def _best_f1_from_pr(y_true: np.ndarray, proba: np.ndarray) -> Tuple[float, float, float, float]:
    """
    å¾ PR æ›²ç·šæ‰¾ F1* èˆ‡å°æ‡‰ thresholdã€Pã€Rã€‚
    å›å‚³: (best_f1, best_thr, best_precision, best_recall)
    """
    precision, recall, thr = precision_recall_curve(y_true, proba)
    f1s = 2 * precision[:-1] * recall[:-1] / (precision[:-1] + recall[:-1] + 1e-12)
    idx = int(np.nanargmax(f1s))
    return float(f1s[idx]), float(thr[idx]), float(precision[idx]), float(recall[idx])

def _prepare_xy(features_path: str, alerts_path: str, mode: str) -> Tuple[pd.DataFrame, pd.Series]:
    """ç”± features + alerts æº–å‚™ X, yï¼›mode âˆˆ {'in','out','both'} æ§åˆ¶å–å“ªäº›ç‰¹å¾µã€‚"""
    feat = pd.read_csv(features_path)
    alerts = pd.read_csv(alerts_path)

    # å°é½Š acct æ¬„ä½
    def _acct_c(df: pd.DataFrame, candidates=None):
        candidates = candidates or ["acct", "acct_id", "account", "account_id", "id"]
        for c in candidates:
            if c in df.columns:
                return c
        raise ValueError("ç„¡ acct æ¬„ä½")

    feat = feat.rename(columns={_acct_c(feat, ["acct","acct_id","account","id"]): "acct"})
    alerts = alerts.rename(columns={_acct_c(alerts, ["acct","acct_id","account","account_id","id"]): "acct"})
    alerts = alerts.drop_duplicates(subset=["acct"]).copy()
    alerts["label"] = 1

    df = feat.merge(alerts[["acct","label"]], on="acct", how="left")
    df["label"] = df["label"].fillna(0).astype(int)

    # ä¾ mode ç¯©ç‰¹å¾µ
    ALL = [c for c in df.columns if c not in ("acct","label")]
    IN  = [c for c in ALL if c.startswith("in_") or c.startswith("in") and ("_w" in c or "_iat" in c or "burst" in c)]
    OUT = [c for c in ALL if c.startswith("out_") or c.startswith("out") and ("_w" in c or "_iat" in c or "burst" in c)]

    # æ›´ç©©å¥ä¸€é»ï¼šç”¨åŒ…å«å­—çœ¼ä¾†æŒ‘
    def _pick(cols: List[str], keywords: List[str]) -> List[str]:
        sel = []
        for c in cols:
            cc = c.lower()
            if any(k in cc for k in keywords):
                sel.append(c)
        return sorted(set(sel))

    in_keys  = ["in_", "in", "night", "weekend", "iat", "burst", "unique_from", "amt_", "txn_count"]
    out_keys = ["out_", "out", "night", "weekend", "iat", "burst", "unique_to", "amt_", "txn_count"]

    in_feats  = _pick(ALL, in_keys)
    out_feats = _pick(ALL, out_keys)

    if mode == "in":
        cols = in_feats
    elif mode == "out":
        cols = out_feats
    elif mode == "both":
        cols = sorted(set(in_feats) | set(out_feats))
    else:
        raise ValueError("mode must be in/out/both")

    X = df[cols].copy()
    y = df["label"].copy()
    return X, y

@dataclass
class ExpResult:
    mode: str
    imb: str
    n_feat: int
    ap: float
    f1_star: float
    thr: float
    precision: float
    recall: float
    pos_at_best_pct: float
    best_iteration: int
    device: str

# ---------------- Core ----------------
def _train_one(X: pd.DataFrame, y: pd.Series, imbalance: str) -> Tuple[xgb.Booster, Dict]:
    """
    è¨“ç·´ä¸€å€‹æ¨¡å‹ï¼›imbalance âˆˆ {'sqrt','full'} æ§åˆ¶ scale_pos_weightã€‚
    å›å‚³ booster èˆ‡è¨“ç·´è³‡è¨Šï¼ˆå« best_iterationï¼‰
    """
    X = _encode_object_columns(X).replace([np.inf, -np.inf], np.nan).fillna(0)

    pos = int((y == 1).sum())
    neg = int((y == 0).sum())
    scale_full = max(1.0, neg / max(1, pos))
    scale_sqrt = float(np.sqrt(scale_full))
    scale = scale_sqrt if imbalance == "sqrt" else scale_full

    Xtr, Xva, ytr, yva = train_test_split(X, y, stratify=y, test_size=0.2, random_state=42)

    dtr = xgb.DMatrix(Xtr, label=ytr, feature_names=X.columns.tolist())
    dva = xgb.DMatrix(Xva, label=yva, feature_names=X.columns.tolist())

    dev = _pick_device_params()
    params = {
        "objective": "binary:logistic",
        "eval_metric": "aucpr",
        "eta": 0.08,
        "max_depth": 8,
        "min_child_weight": 5,
        "subsample": 0.8,
        "colsample_bytree": 0.8,
        "scale_pos_weight": scale,
        "max_bin": 256,
        "reg_alpha": 0.1,
        "reg_lambda": 1.0,
        **dev,
    }
    print(f"ğŸ§ª device={dev['device']} tree_method={dev['tree_method']}  |  scale_pos_weight={scale:.2f}")

    bst = xgb.train(
        params,
        dtr,
        num_boost_round=1000,
        evals=[(dtr, "train"), (dva, "val")],
        early_stopping_rounds=80,
        verbose_eval=100,
    )
    info = {
        "best_iteration": int(bst.best_iteration),
        "device": dev["device"],
        "X_val": Xva,
        "y_val": yva,
        "dval": dva,
    }
    return bst, info

def _eval_one(bst: xgb.Booster, info: Dict) -> Tuple[float, float, float, float, float, float]:
    """è¨ˆç®— AP èˆ‡æœ€ä½³ F1ï¼ˆå« thr / P / R / pos%ï¼‰ã€‚"""
    dva = info["dval"]
    y_val = info["y_val"].values
    proba = bst.predict(dva, iteration_range=(0, bst.best_iteration + 1))

    ap = average_precision_score(y_val, proba)
    f1_star, thr, prec, rec = _best_f1_from_pr(y_val, proba)
    pos_rate = float((proba >= thr).mean() * 100.0)
    return ap, f1_star, thr, prec, rec, pos_rate

def run_experiments(features_path: str, alerts_path: str, out_dir: str, sort_by: str = "ap"):
    """
    è·‘ 6 çµ„ï¼šmode âˆˆ {in, out, both} Ã— imb âˆˆ {sqrt, full}
    ç”¢å‡º outputs/exp_summary.csv ä¸¦å°è¡¨æ ¼ï¼›sort_by å¯é¸ 'ap' æˆ– 'f1_star'
    """
    Path(out_dir).mkdir(parents=True, exist_ok=True)
    results: List[ExpResult] = []

    for mode in ["in", "out", "both"]:
        # æº–å‚™è³‡æ–™
        X, y = _prepare_xy(features_path, alerts_path, mode)

        print("Label åˆ†å¸ƒï¼š\n", y.value_counts())
        print(f"DEBUG | mode={mode} | X type={type(X)} shape={X.shape} dtypes={X.dtypes.nunique()}")

        for imb in ["sqrt", "full"]:
            bst, info = _train_one(X, y, imbalance=imb)
            ap, f1s, thr, p, r, pos_pct = _eval_one(bst, info)

            print(f"âœ“ å®Œæˆ mode={mode:>4}  imb={imb:<5}  AP={ap:.5f}  F1*={f1s:.4f}  "
                  f"P={p:.4f} R={r:.4f}  pos@best={pos_pct:.4f}%")

            res = ExpResult(
                mode=mode,
                imb=imb,
                n_feat=X.shape[1],
                ap=ap,
                f1_star=f1s,
                thr=thr,
                precision=p,
                recall=r,
                pos_at_best_pct=pos_pct,
                best_iteration=info["best_iteration"],
                device=info["device"],
            )
            results.append(res)

    # å½™æ•´è¡¨
    df_res = pd.DataFrame([asdict(r) for r in results])
    sort_key = "ap" if sort_by.lower() == "ap" else "f1_star"
    df_res_sorted = df_res.sort_values(sort_key, ascending=False).reset_index(drop=True)

    # å„²å­˜ CSV
    out_csv = Path(out_dir) / "exp_summary.csv"
    df_res_sorted.to_csv(out_csv, index=False, encoding="utf-8")

    # é¡¯ç¤ºè¡¨æ ¼
    print("\n================== å¯¦é©—å½™æ•´ï¼ˆæ’åºä¾ {}ï¼‰ ==================".format(sort_key.upper()))
    print(df_res_sorted.to_string(index=False, formatters={
        "ap": lambda v: f"{v:.5f}",
        "f1_star": lambda v: f"{v:.4f}",
        "thr": lambda v: f"{v:.6f}",
        "precision": lambda v: f"{v:.4f}",
        "recall": lambda v: f"{v:.4f}",
        "pos_at_best_pct": lambda v: f"{v:.4f}%",
    }))
    best = df_res_sorted.iloc[0].to_dict()
    print("\nğŸ† æœ€ä½³çµ„åˆ -> mode={mode}, imb={imb}, AP={ap:.5f}, F1*={f1_star:.4f}, "
          "P={precision:.4f}, R={recall:.4f}, thr={thr:.6f}, pos@best={pos_at_best_pct:.4f}%"
          .format(**best))
    print(f"ğŸ“„ å·²å„²å­˜å½™æ•´è¡¨ï¼š{out_csv}")

# ---------------- CLI ----------------
def parse_args():
    p = argparse.ArgumentParser(description="Run ablation experiments and summarize results.")
    p.add_argument("--features", default="feature_data/account_features.csv")
    p.add_argument("--alerts",   default="dataset/acct_alert.csv")
    p.add_argument("--out",      default="outputs")
    p.add_argument("--sort-by",  default="ap", choices=["ap","f1"], help="å½™æ•´è¡¨æ’åºä¾æ“š")
    return p.parse_args()

if __name__ == "__main__":
    args = parse_args()
    sort_by = "ap" if args.sort_by.lower() == "ap" else "f1"
    run_experiments(args.features, args.alerts, args.out, sort_by=("ap" if sort_by=="ap" else "f1_star"))
