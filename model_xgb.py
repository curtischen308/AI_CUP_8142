# model_xgb.py
import numpy as np
import pandas as pd
import xgboost as xgb
from sklearn.model_selection import train_test_split
from sklearn.metrics import precision_recall_curve
from joblib import dump, load
from pathlib import Path


def _encode_object_columns(df: pd.DataFrame) -> pd.DataFrame:
    """å°‡ object/string æ¬„ä½ä»¥ factorize è½‰æˆæ•´æ•¸ï¼Œé¿å… XGBoost DMatrix å ± dtype éŒ¯ã€‚"""
    for col in df.columns:
        if df[col].dtype == "object":
            df[col] = pd.factorize(df[col])[0]
    return df


def _pick_device_params():
    """
    XGBoost 2.x å»ºè­°ï¼šGPU ç”¨ device='cuda' + tree_method='hist'
    è‹¥ä¸å¯ç”¨å‰‡é€€å› CPUã€‚
    """
    try:
        params = {"objective": "binary:logistic", "tree_method": "hist", "device": "cuda"}
        dm = xgb.DMatrix(np.array([[0.0], [1.0]]), label=np.array([0, 1]))
        xgb.train(params, dm, num_boost_round=1)
        return {"tree_method": "hist", "device": "cuda"}
    except Exception:
        return {"tree_method": "hist", "device": "cpu"}


def train_model(train_path="train.csv", model_path="model_xgb.joblib"):
    data = pd.read_csv(train_path)

    if "acct" not in data.columns or "label" not in data.columns:
        raise ValueError("train.csv å¿…é ˆåŒ…å« 'acct' èˆ‡ 'label' æ¬„ä½")

    X = data.drop(columns=["acct", "label"]).copy()
    y = data["label"].astype(int)

    # object â†’ æ•´æ•¸ï¼›NaN/Infâ†’0
    X = _encode_object_columns(X)
    X = X.replace([np.inf, -np.inf], np.nan).fillna(0)

    pos, neg = int((y == 1).sum()), int((y == 0).sum())
    if pos == 0:
        raise ValueError("è¨“ç·´è³‡æ–™æ²’æœ‰æ­£æ¨£æœ¬ï¼Œç„¡æ³•è¨“ç·´ã€‚")
    scale = max(1.0, neg / max(1, pos))
    print(f"æ­£æ¨£æœ¬: {pos}, è² æ¨£æœ¬: {neg}, scale_pos_weight={scale:.2f}")

    X_train, X_val, y_train, y_val = train_test_split(
        X, y, stratify=y, test_size=0.2, random_state=42
    )

    dtrain = xgb.DMatrix(X_train, label=y_train, feature_names=X.columns.tolist())
    dval   = xgb.DMatrix(X_val,   label=y_val,   feature_names=X.columns.tolist())

    dev = _pick_device_params()
    params = {
        "objective": "binary:logistic",
        "eval_metric": ["logloss", "aucpr"],   # å°ä¸å¹³è¡¡æ›´æ•æ„Ÿ
        "eta": 0.09,                           # é™å­¸ç¿’ç‡ï¼Œæå‡æ³›åŒ–
        "max_depth": 7,
        "min_child_weight": 5,
        "subsample": 0.7,
        "colsample_bytree": 0.7,
        "scale_pos_weight": scale,
        "max_bin": 256,
        "reg_alpha": 0.1,
        "reg_lambda": 1.0,
        **dev,
    }
    print(f"ä½¿ç”¨ device={dev['device']} tree_method={dev['tree_method']}")

    bst = xgb.train(
        params,
        dtrain,
        num_boost_round=1000,
        evals=[(dtrain, "train"), (dval, "val")],
        early_stopping_rounds=100,
        verbose_eval=50,
    )

    # PR æ›²ç·šæŒ‘æœ€ä½³ F1ï¼ˆèˆ‡ threshold å°é½Šï¼‰
    proba = bst.predict(dval, iteration_range=(0, bst.best_iteration + 1))
    prec, rec, thr = precision_recall_curve(y_val, proba)
    f1s = 2 * prec[:-1] * rec[:-1] / (prec[:-1] + rec[:-1] + 1e-12)
    best_idx = int(np.nanargmax(f1s))
    best_thr = float(thr[best_idx])
    print(f"æœ€ä½³é–¾å€¼={best_thr:.4f}, é©—è­‰é›† F1={f1s[best_idx]:.4f}")

    # å­˜æ¨¡å‹ + ç‰¹å¾µå + é–¾å€¼
    dump({"model": bst, "threshold": best_thr, "feature_cols": X.columns.tolist()}, model_path)
    print(f"âœ… æ¨¡å‹å·²å­˜æª”åˆ° {model_path}")

    # å­˜ç‰¹å¾µé‡è¦åº¦ï¼ˆæ–¹ä¾¿ä½ èª¿åƒ/é¸ç‰¹å¾µï¼‰
    try:
        fmap = pd.DataFrame({
            "feature": X.columns,
            "gain":    bst.get_score(importance_type="gain"),
            "weight":  bst.get_score(importance_type="weight"),
            "cover":   bst.get_score(importance_type="cover"),
        }).fillna(0)
        fmap.to_csv("outputs/feature_importance.csv", index=False, encoding="utf-8")
        print("ğŸ“ å·²è¼¸å‡ºç‰¹å¾µé‡è¦åº¦åˆ° outputs/feature_importance.csv")
    except Exception:
        pass

def predict(
    test_path: str = "test.csv",
    model_path: str = "model_xgb.joblib",
    output_path: str = "prediction.csv",
    thresholds: list[float] | None = None,
    topk_show: int = 10
):
    """
    æ¨è«– + è¨ºæ–·è¼¸å‡º
    - thresholds: é¡å¤–æƒ³çœ‹çš„å¤šçµ„é–¾å€¼ï¼ˆæœƒå°å‡ºæ¯å€‹é–¾å€¼çš„é™½æ€§æ•¸èˆ‡æ¯”ä¾‹ï¼Œä¸¦åœ¨ CSV åŠ æ¬„ä½ pred_tXXï¼‰
    - topk_show: æœƒå°å‡ºæ©Ÿç‡æœ€é«˜çš„å‰ k ç­†ï¼Œæ–¹ä¾¿è‚‰çœ¼æª¢æŸ¥
    """
    if thresholds is None:
        thresholds = [0.5, 0.3, 0.2, 0.1]

    bundle = load(model_path)
    bst = bundle["model"]
    best_thr = float(bundle["threshold"])
    feature_cols = bundle["feature_cols"]

    df = pd.read_csv(test_path)

    # å®¹éŒ¯ï¼šacct / acct_id / account / account_id / id çš†å¯
    if "acct" not in df.columns:
        for c in ["acct_id", "account", "account_id", "id"]:
            if c in df.columns:
                df = df.rename(columns={c: "acct"})
                break
    if "acct" not in df.columns:
        raise ValueError("æ¸¬è©¦æª”å¿…é ˆåŒ…å« acctï¼ˆæˆ– acct_id/account/account_id/id å…¶ä¸­ä¹‹ä¸€ï¼‰")

    # å°é½Šç‰¹å¾µï¼ˆç¼ºçš„è£œ 0ï¼Œå¤šçš„ä¸Ÿæ‰ï¼‰
    X_new = df.reindex(columns=feature_cols, fill_value=0)
    X_new = _encode_object_columns(X_new)
    X_new = X_new.replace([np.inf, -np.inf], np.nan).fillna(0)

    dnew = xgb.DMatrix(X_new)
    proba = bst.predict(dnew)

    # === è¨ºæ–·ï¼šåˆ†æ•¸çµ±è¨ˆ ===
    print("\nğŸ“ˆ Score stats (prediction probabilities)")
    print(f"min={proba.min():.6f}  p50={np.median(proba):.6f}  mean={proba.mean():.6f}  "
          f"p90={np.quantile(proba, 0.9):.6f}  max={proba.max():.6f}")

    # ä½¿ç”¨è¨“ç·´æ™‚çš„æœ€ä½³é–¾å€¼ï¼ˆä¸»è¼¸å‡ºï¼‰
    pred = (proba >= best_thr).astype(int)
    print(f"\nğŸ¯ Using best threshold from training: thr={best_thr:.4f}")
    print(f"positives={pred.sum()} / {len(pred)}  ({pred.mean()*100:.3f}%)")

    # é¡å¤–å¤šçµ„é–¾å€¼
    extra_cols = {}
    print("\nğŸ” Extra thresholds inspection:")
    for thr in thresholds:
        p = (proba >= thr).astype(int)
        ratio = p.mean() * 100
        print(f" - thr={thr:>5.2f} -> positives={p.sum():>6} / {len(p)}  ({ratio:6.3f}%)")
        extra_cols[f"pred_t{str(thr).replace('.','_')}"] = p

    # æ©Ÿç‡æœ€é«˜çš„å‰ k ç­†
    if topk_show > 0:
        top_idx = np.argsort(-proba)[:topk_show]
        print(f"\nğŸ‘€ Top {topk_show} by probability:")
        for i in top_idx:
            print(f"  acct={df.loc[i, 'acct']}, proba={proba[i]:.6f}")

    # === è¼¸å‡º CSV ===
    out = pd.DataFrame({"acct": df["acct"], "proba": proba, "pred": pred})
    for c, v in extra_cols.items():
        out[c] = v

    Path(output_path).parent.mkdir(parents=True, exist_ok=True)
    out.to_csv(output_path, index=False, encoding="utf-8")
    print(f"\nâœ… é æ¸¬è¼¸å‡ºåˆ° {output_path}")