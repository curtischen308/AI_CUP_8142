# model_xgb.py
import numpy as np
import pandas as pd
import xgboost as xgb
from sklearn.model_selection import train_test_split
from sklearn.metrics import precision_recall_curve, confusion_matrix, roc_auc_score
from joblib import dump, load
from pathlib import Path
import warnings

warnings.filterwarnings("ignore", category=UserWarning)


def _encode_object_columns(df: pd.DataFrame) -> pd.DataFrame:
    """æŠŠ object é¡å‹ç”¨ factorize è½‰æˆæ•´æ•¸ï¼Œé¿å… DMatrix dtype å•é¡Œã€‚"""
    for col in df.columns:
        if df[col].dtype == "object":
            df[col] = pd.factorize(df[col])[0]
    return df


def _pick_device_params():
    """
    å„ªå…ˆç”¨ GPUï¼ˆdevice='cuda' + tree_method='hist'ï¼‰ï¼Œä¸å¯ç”¨æ‰é€€å› CPUã€‚
    """
    try:
        params = {"objective": "binary:logistic", "tree_method": "hist", "device": "cuda"}
        dm = xgb.DMatrix(np.array([[0.0], [1.0]]), label=np.array([0, 1]))
        xgb.train(params, dm, num_boost_round=1)
        return {"tree_method": "hist", "device": "cuda"}
    except Exception:
        return {"tree_method": "hist", "device": "cpu"}


def _choose_threshold(y_true: np.ndarray, proba: np.ndarray, pos_rate_train: float) -> float:
    """
    æ™ºæ…§é¸ thresholdï¼Œé¿å…å…¨ 0ï¼š
      1) å…ˆç”¨ PR æ›²ç·šé¸ F1 æœ€ä½³
      2) è‹¥ä»å…¨ 0ï¼Œé€€å›ä»¥è¨“ç·´é›†é™½æ€§ç‡ç‚ºç›®æ¨™çš„åˆ†ä½æ•¸é–¾å€¼ï¼ˆ*1.5 bufferï¼‰
    """
    prec, rec, thr = precision_recall_curve(y_true, proba)
    f1s = 2 * prec[:-1] * rec[:-1] / (prec[:-1] + rec[:-1] + 1e-12)

    best_idx = int(np.nanargmax(f1s))
    best_thr = float(thr[best_idx])

    pred_a = (proba >= best_thr).astype(int)
    if pred_a.sum() == 0:
        q = max(1e-6, min(0.02, pos_rate_train * 1.5))
        best_thr = float(np.quantile(proba, 1 - q))

    return best_thr


def _random_undersample(X: pd.DataFrame, y: pd.Series, max_neg_per_pos: int = 500, seed: int = 42):
    """
    åœ¨æ¥µç«¯ä¸å¹³è¡¡æƒ…æ³ä¸‹ï¼Œåšç°¡å–®éš¨æ©Ÿä¸‹æ¡æ¨£ï¼ˆä¸éœ€é¡å¤–å¥—ä»¶ï¼‰ä»¥åŠ é€Ÿ&ç©©å®šã€‚
    """
    pos_idx = y[y == 1].index
    neg_idx = y[y == 0].index

    if len(pos_idx) == 0 or len(neg_idx) == 0:
        return X, y

    cap = int(min(len(neg_idx), max_neg_per_pos * max(1, len(pos_idx))))
    if cap >= len(neg_idx):
        return X, y

    rng = np.random.default_rng(seed)
    keep_neg = rng.choice(neg_idx, size=cap, replace=False)
    keep_idx = np.concatenate([pos_idx.values, keep_neg])
    keep_idx.sort()
    return X.loc[keep_idx], y.loc[keep_idx]


def train_model(train_path="train.csv", model_path="model_xgb.joblib"):
    data = pd.read_csv(train_path)

    if "acct" not in data.columns or "label" not in data.columns:
        raise ValueError("train.csv å¿…é ˆåŒ…å« 'acct' èˆ‡ 'label' æ¬„ä½")

    X_full = data.drop(columns=["acct", "label"]).copy()
    y_full = data["label"].astype(int)

    # æ•´ç†ç‰¹å¾µ
    X_full = _encode_object_columns(X_full)
    X_full = X_full.replace([np.inf, -np.inf], np.nan).fillna(0)

    pos, neg = int((y_full == 1).sum()), int((y_full == 0).sum())
    if pos == 0:
        raise ValueError("è¨“ç·´è³‡æ–™æ²’æœ‰æ­£æ¨£æœ¬ï¼Œç„¡æ³•è¨“ç·´ã€‚")
    pos_rate_train = pos / (pos + neg + 1e-9)
    scale = float(np.sqrt(neg / max(1, pos)))  # æ¯”å‚³çµ± neg/pos æ›´æº«å’Œ
    print(f"æ­£æ¨£æœ¬: {pos}, è² æ¨£æœ¬: {neg}, pos_rate={pos_rate_train:.6f}, scale_pos_weight={scale:.2f}")

    # ä¸‹æ¡æ¨£ï¼ˆæ¥µç«¯ä¸å¹³è¡¡æ‰å•Ÿå‹•ï¼‰
    if neg / max(1, pos) > 1000:
        X_bal, y_bal = _random_undersample(X_full, y_full, max_neg_per_pos=800)
        print(f"ğŸ”§ å•Ÿç”¨éš¨æ©Ÿä¸‹æ¡æ¨£ï¼š{len(y_full)} -> {len(y_bal)}")
    else:
        X_bal, y_bal = X_full, y_full

    X_train, X_val, y_train, y_val = train_test_split(
        X_bal, y_bal, stratify=y_bal, test_size=0.2, random_state=42
    )

    dtrain = xgb.DMatrix(X_train, label=y_train, feature_names=X_bal.columns.tolist())
    dval   = xgb.DMatrix(X_val,   label=y_val,   feature_names=X_bal.columns.tolist())

    dev = _pick_device_params()
    params = {
        "objective": "binary:logistic",
        "eval_metric": "aucpr",     # PR-AUC å°ä¸å¹³è¡¡æ›´æ•æ„Ÿ
        "eta": 0.1,
        "max_depth": 8,
        "min_child_weight": 5,
        "subsample": 0.7,
        "colsample_bytree": 0.7,
        "scale_pos_weight": scale,
        "max_bin": 256,
        "reg_alpha": 0.2,
        "reg_lambda": 1.0,
        **dev,
    }
    print(f"ä½¿ç”¨ device={dev['device']} tree_method={dev['tree_method']}")

    bst = xgb.train(
        params,
        dtrain,
        num_boost_round=2000,
        evals=[(dtrain, "train"), (dval, "val")],
        early_stopping_rounds=150,
        verbose_eval=100,
    )

    # é©—è­‰é›†æ©Ÿç‡èˆ‡é–¾å€¼
    proba_val = bst.predict(dval, iteration_range=(0, bst.best_iteration + 1))
    best_thr = _choose_threshold(y_val.values, proba_val, pos_rate_train)

    # é©—è­‰æŒ‡æ¨™
    y_pred_val = (proba_val >= best_thr).astype(int)
    tn, fp, fn, tp = confusion_matrix(y_val, y_pred_val).ravel()
    prec = tp / (tp + fp + 1e-12)
    rec  = tp / (tp + fn + 1e-12)
    f1   = 2 * prec * rec / (prec + rec + 1e-12)
    pr_auc = roc_auc_score(y_val, proba_val)  # é€™è£¡å…¶å¯¦æ˜¯ ROC-AUCï¼›PR-AUC è«‹ç”¨ precision_recall_curve+auc
    print(f"é¸å®šé–¾å€¼={best_thr:.6f} | é©—è­‰: TP={tp} FP={fp} TN={tn} FN={fn} | P={prec:.4f} R={rec:.4f} F1={f1:.4f} | ROC-AUC={pr_auc:.4f}")

    # å­˜æ¨¡å‹ + é–¾å€¼ + ç‰¹å¾µå + è¨“ç·´é™½æ€§ç‡
    dump({
        "model": bst,
        "threshold": best_thr,
        "feature_cols": X_bal.columns.tolist(),
        "pos_rate_train": pos_rate_train
    }, model_path)
    print(f"âœ… æ¨¡å‹å·²å­˜æª”åˆ° {model_path}")

    # ç‰¹å¾µé‡è¦åº¦ï¼ˆæ–¹ä¾¿èª¿åƒ/é¸ç‰¹å¾µï¼‰
    try:
        fmap_gain   = bst.get_score(importance_type="gain")
        fmap_weight = bst.get_score(importance_type="weight")
        fmap_cover  = bst.get_score(importance_type="cover")
        all_feats = list({*X_bal.columns.tolist(), *fmap_gain.keys(), *fmap_weight.keys(), *fmap_cover.keys()})
        rows = []
        for f in all_feats:
            rows.append({
                "feature": f,
                "gain":   fmap_gain.get(f, 0.0),
                "weight": fmap_weight.get(f, 0.0),
                "cover":  fmap_cover.get(f, 0.0),
            })
        fmap = pd.DataFrame(rows)
        Path("outputs").mkdir(exist_ok=True, parents=True)
        fmap.sort_values("gain", ascending=False).to_csv("outputs/feature_importance.csv", index=False, encoding="utf-8")
        print("ğŸ“ å·²è¼¸å‡ºç‰¹å¾µé‡è¦åº¦åˆ° outputs/feature_importance.csv")
    except Exception:
        pass


def predict(
    test_path: str = "test.csv",
    model_path: str = "model_xgb.joblib",
    output_path: str = "prediction.csv",
    thresholds: list[float] | None = None,
    topk_show: int = 10,
    target_pos_rate: float | None = None,
    min_positives: int = 1,  # æœ€å°‘é™½æ€§æ•¸ï¼ˆæœ€å¾Œä¿éšªï¼‰
):
    """
    æ¨è«– + è¨ºæ–·è¼¸å‡º
    - thresholds: é¡å¤–æƒ³çœ‹çš„å¤šçµ„é–¾å€¼ï¼ˆæœƒå°å‡ºæ¯å€‹é–¾å€¼çš„é™½æ€§æ•¸èˆ‡æ¯”ä¾‹ï¼Œä¸¦åœ¨ CSV åŠ æ¬„ä½ pred_tXXï¼‰
    - topk_show: æœƒå°å‡ºæ©Ÿç‡æœ€é«˜çš„å‰ k ç­†ï¼Œæ–¹ä¾¿è‚‰çœ¼æª¢æŸ¥
    - target_pos_rate: æƒ³è¦çš„é™½æ€§æ¯”ä¾‹ï¼Œæ¯”å¦‚ 0.002 -> 0.2%ï¼Œæœƒç”¨åˆ†ä½æ•¸æ±‚ threshold è¦†å¯«
    - min_positives: æœ€å°‘é™½æ€§æ•¸ï¼ˆTop-K ä¿åº•ï¼‰
    """
    if thresholds is None:
        thresholds = [0.5, 0.3, 0.2, 0.1]

    bundle = load(model_path)
    bst = bundle["model"]
    best_thr = float(bundle.get("threshold", 0.5))
    feature_cols = bundle["feature_cols"]
    pos_rate_train = float(bundle.get("pos_rate_train", 0.0005))

    df = pd.read_csv(test_path)

    # å®¹éŒ¯ï¼šacct / acct_id / account / account_id / id çš†å¯
    if "acct" not in df.columns:
        for c in ["acct_id", "account", "account_id", "id"]:
            if c in df.columns:
                df = df.rename(columns={c: "acct"})
                break
    if "acct" not in df.columns:
        raise ValueError("æ¸¬è©¦æª”å¿…é ˆåŒ…å« acctï¼ˆæˆ– acct_id/account/account_id/id å…¶ä¸­ä¹‹ä¸€ï¼‰")

    # å°é½Šç‰¹å¾µï¼ˆç¼ºçš„è£œ 0ï¼Œå¤šçš„ä¸Ÿæ‰ï¼‰
    X_new = df.reindex(columns=feature_cols, fill_value=0)
    for col in X_new.columns:
        if X_new[col].dtype == "object":
            X_new[col] = pd.factorize(X_new[col])[0]
    X_new = X_new.replace([np.inf, -np.inf], np.nan).fillna(0)

    dnew = xgb.DMatrix(X_new, feature_names=feature_cols)
    try:
        proba = bst.predict(dnew, iteration_range=(0, bst.best_iteration + 1))
    except Exception:
        proba = bst.predict(dnew)

    # === è¨ºæ–·ï¼šåˆ†æ•¸åˆ†ä½ˆ ===
    print("\nğŸ“ˆ Score stats (prediction probabilities)")
    print(f"min={proba.min():.6f}  p50={np.median(proba):.6f}  mean={proba.mean():.6f}  "
          f"p90={np.quantile(proba, 0.9):.6f}  max={proba.max():.6f}")

    # 1) é è¨­ç”¨è¨“ç·´å¾—åˆ°çš„æœ€ä½³ threshold
    use_thr = float(best_thr)

    # 2) è‹¥æœ‰æŒ‡å®šç›®æ¨™é™½æ€§ç‡ï¼Œæ”¹ç”¨åˆ†ä½æ•¸è¦†å¯« threshold
    if target_pos_rate is not None and target_pos_rate > 0:
        q = max(1e-5, min(0.05, float(target_pos_rate)))
        use_thr = float(np.quantile(proba, 1 - q))
        print(f"ğŸ”§ è¦†å¯« threshold ä»¥é”ç›®æ¨™é™½æ€§ç‡ ~{q*100:.3f}% -> thr={use_thr:.6f}")

    pred = (proba >= use_thr).astype(int)
    print(f"\nğŸ¯ Using threshold: thr={use_thr:.6f}")
    print(f"positives={pred.sum()} / {len(pred)}  ({pred.mean()*100:.3f}%)")

    # 3) è‹¥ä»éåº¦ä¿å®ˆï¼ˆå¹¾ä¹å…¨ 0ï¼‰ï¼ŒæŒ‰ã€Œè¨“ç·´é™½æ€§ç‡çš„ 1.5 å€ã€åˆ†ä½æ•¸å›é€€
    if pred.sum() < max(1, min_positives):
        fallback_rate = max(1e-6, min(0.05, pos_rate_train * 1.5))
        new_thr = float(np.quantile(proba, 1 - fallback_rate))
        pred_fb = (proba >= new_thr).astype(int)
        print(f"âš ï¸  å…¨ 0 é¢¨éšªï¼Œå›é€€åˆ†ä½æ•¸ thresholdï¼š{new_thr:.6f} (target~{fallback_rate*100:.4f}%) "
              f"-> positives={pred_fb.sum()} ({pred_fb.mean()*100:.3f}%)")
        if pred_fb.sum() >= max(1, min_positives):
            use_thr = new_thr
            pred = pred_fb

    # 4) è‹¥é‚„æ˜¯ä¸å¤ ï¼Œå†å¼·åˆ¶å– Top-Kï¼ˆä¿åº•ä¸ç‚º 0ï¼‰
    if pred.sum() < max(1, min_positives) and proba.size > 0:
        need = max(1, min_positives) - int(pred.sum())
        top_idx = np.argsort(-proba)[:need]
        pred[top_idx] = 1
        print(f"ğŸ›Ÿ å¼·åˆ¶å– Top-{need} åšé™½æ€§ä¿åº•ï¼›new positives={pred.sum()} ({pred.mean()*100:.3f}%)")
        use_thr = float(min(proba[top_idx].min(), use_thr))

    # é¡å¤–å¤šçµ„é–¾å€¼è¨ºæ–·
    extra_cols = {}
    print("\nğŸ” Extra thresholds inspection:")
    for thr in thresholds:
        p = (proba >= thr).astype(int)
        ratio = p.mean() * 100
        print(f" - thr={thr:>5.2f} -> positives={p.sum():>6} / {len(p)}  ({ratio:6.3f}%)")
        extra_cols[f"pred_t{str(thr).replace('.','_')}"] = p

    # æ©Ÿç‡æœ€é«˜çš„å‰ k ç­†
    if topk_show > 0:
        top_idx = np.argsort(-proba)[:topk_show]
        print(f"\nğŸ‘€ Top {topk_show} by probability:")
        for i in top_idx:
            print(f"  acct={df.loc[i, 'acct']}, proba={proba[i]:.6f}")

    # === è¼¸å‡º CSV ===
    out = pd.DataFrame({"acct": df["acct"], "proba": proba, "pred": pred})
    for c, v in extra_cols.items():
        out[c] = v

    Path(output_path).parent.mkdir(parents=True, exist_ok=True)
    out.to_csv(output_path, index=False, encoding="utf-8")
    print(f"\nâœ… é æ¸¬è¼¸å‡ºåˆ° {output_path}")
